Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     
_________________________________________________________________
activation_1 (Activation)    (None, 55, 55, 96)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 27, 27, 96)        384       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   
_________________________________________________________________
activation_2 (Activation)    (None, 17, 17, 256)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    
_________________________________________________________________
activation_3 (Activation)    (None, 6, 6, 384)         0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 6, 6, 384)         1536      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   
_________________________________________________________________
activation_4 (Activation)    (None, 4, 4, 384)         0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 4, 4, 384)         1536      
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    
_________________________________________________________________
activation_5 (Activation)    (None, 2, 2, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              1052672   
_________________________________________________________________
activation_6 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
activation_7 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
_________________________________________________________________
activation_8 (Activation)    (None, 1000)              0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 1000)              0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 1000)              4000      
_________________________________________________________________
dense_4 (Dense)              (None, 38)                38038     
_________________________________________________________________
activation_9 (Activation)    (None, 38)                0         
=================================================================
Total params: 28,117,790
Trainable params: 28,096,654
Non-trainable params: 21,136
_________________________________________________________________

lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)
csv_logger = CSVLogger('Alexnet_RMSprop.csv')
early_stopper = EarlyStopping(min_delta=0.001,patience=30)
model_checkpoint = ModelCheckpoint('Alexnet_RMSprop.hdf5',monitor = 'val_loss', verbose = 1,save_best_only=True)

model.compile(loss='categorical_crossentropy',
        optimizer="RMSprop",
        metrics=['accuracy'])

model.fit(alexnet_train_data, alexnet_train_label,
              batch_size=12,
              epochs=30,
              validation_data=(alexnet_test_data,alexnet_test_label),
              shuffle=True,callbacks=[lr_reducer,csv_logger,early_stopper,model_checkpoint])
Train on 43444 samples, validate on 10861 samples
Epoch 1/30
43444/43444 [==============================] - 97s 2ms/step - loss: 3.1693 - acc: 0.2037 - val_loss: 3.4828 - val_acc: 0.1437

Epoch 00001: val_loss improved from inf to 3.48281, saving model to Alexnet_RMSprop.hdf5
Epoch 2/30
43444/43444 [==============================] - 91s 2ms/step - loss: 2.4909 - acc: 0.3599 - val_loss: 3.0134 - val_acc: 0.3422

Epoch 00002: val_loss improved from 3.48281 to 3.01344, saving model to Alexnet_RMSprop.hdf5
Epoch 3/30
43444/43444 [==============================] - 91s 2ms/step - loss: 1.9013 - acc: 0.4992 - val_loss: 7.2085 - val_acc: 0.2193

Epoch 00003: val_loss did not improve from 3.01344
Epoch 4/30
43444/43444 [==============================] - 92s 2ms/step - loss: 1.5483 - acc: 0.5839 - val_loss: 2.5153 - val_acc: 0.4919

Epoch 00004: val_loss improved from 3.01344 to 2.51532, saving model to Alexnet_RMSprop.hdf5
Epoch 5/30
43444/43444 [==============================] - 92s 2ms/step - loss: 1.3110 - acc: 0.6400 - val_loss: 1.3546 - val_acc: 0.6708

Epoch 00005: val_loss improved from 2.51532 to 1.35459, saving model to Alexnet_RMSprop.hdf5
Epoch 6/30
43444/43444 [==============================] - 93s 2ms/step - loss: 1.1280 - acc: 0.6931 - val_loss: 1.2350 - val_acc: 0.6991

Epoch 00006: val_loss improved from 1.35459 to 1.23495, saving model to Alexnet_RMSprop.hdf5
Epoch 7/30
43444/43444 [==============================] - 94s 2ms/step - loss: 0.9747 - acc: 0.7298 - val_loss: 1.7414 - val_acc: 0.5425

Epoch 00007: val_loss did not improve from 1.23495
Epoch 8/30
43444/43444 [==============================] - 94s 2ms/step - loss: 0.8561 - acc: 0.7646 - val_loss: 1.3186 - val_acc: 0.7199

Epoch 00008: val_loss did not improve from 1.23495
Epoch 9/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.6179 - acc: 0.8251 - val_loss: 1.1004 - val_acc: 0.7062

Epoch 00009: val_loss improved from 1.23495 to 1.10035, saving model to Alexnet_RMSprop.hdf5
Epoch 10/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.5528 - acc: 0.8431 - val_loss: 0.5947 - val_acc: 0.8636

Epoch 00010: val_loss improved from 1.10035 to 0.59472, saving model to Alexnet_RMSprop.hdf5
Epoch 11/30
43444/43444 [==============================] - 95s 2ms/step - loss: 0.5212 - acc: 0.8530 - val_loss: 0.5163 - val_acc: 0.8661

Epoch 00011: val_loss improved from 0.59472 to 0.51632, saving model to Alexnet_RMSprop.hdf5
Epoch 12/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.4815 - acc: 0.8644 - val_loss: 0.4790 - val_acc: 0.8728

Epoch 00012: val_loss improved from 0.51632 to 0.47898, saving model to Alexnet_RMSprop.hdf5
Epoch 13/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.4525 - acc: 0.8728 - val_loss: 0.6279 - val_acc: 0.8276

Epoch 00013: val_loss did not improve from 0.47898
Epoch 14/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.4174 - acc: 0.8817 - val_loss: 0.5522 - val_acc: 0.8533

Epoch 00014: val_loss did not improve from 0.47898
Epoch 15/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.3410 - acc: 0.9009 - val_loss: 0.2977 - val_acc: 0.9282

Epoch 00015: val_loss improved from 0.47898 to 0.29769, saving model to Alexnet_RMSprop.hdf5
Epoch 16/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.3214 - acc: 0.9063 - val_loss: 0.2744 - val_acc: 0.9318

Epoch 00016: val_loss improved from 0.29769 to 0.27441, saving model to Alexnet_RMSprop.hdf5
Epoch 17/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.3206 - acc: 0.9076 - val_loss: 0.2732 - val_acc: 0.9329

Epoch 00017: val_loss improved from 0.27441 to 0.27316, saving model to Alexnet_RMSprop.hdf5
Epoch 18/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.3052 - acc: 0.9116 - val_loss: 0.2918 - val_acc: 0.9297

Epoch 00018: val_loss did not improve from 0.27316
Epoch 19/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.2929 - acc: 0.9149 - val_loss: 0.3260 - val_acc: 0.9206

Epoch 00019: val_loss did not improve from 0.27316
Epoch 20/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.2776 - acc: 0.9195 - val_loss: 0.2721 - val_acc: 0.9367

Epoch 00020: val_loss improved from 0.27316 to 0.27209, saving model to Alexnet_RMSprop.hdf5
Epoch 21/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.2665 - acc: 0.9229 - val_loss: 0.2691 - val_acc: 0.9383

Epoch 00021: val_loss improved from 0.27209 to 0.26908, saving model to Alexnet_RMSprop.hdf5
Epoch 22/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.2677 - acc: 0.9231 - val_loss: 0.2447 - val_acc: 0.9412

Epoch 00022: val_loss improved from 0.26908 to 0.24473, saving model to Alexnet_RMSprop.hdf5
Epoch 23/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.2521 - acc: 0.9279 - val_loss: 0.2470 - val_acc: 0.9418

Epoch 00023: val_loss did not improve from 0.24473
Epoch 24/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.2560 - acc: 0.9258 - val_loss: 0.2568 - val_acc: 0.9394

Epoch 00024: val_loss did not improve from 0.24473
Epoch 25/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.2581 - acc: 0.9257 - val_loss: 0.2365 - val_acc: 0.9430

Epoch 00025: val_loss improved from 0.24473 to 0.23651, saving model to Alexnet_RMSprop.hdf5
Epoch 26/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.2489 - acc: 0.9270 - val_loss: 0.2345 - val_acc: 0.9417

Epoch 00026: val_loss improved from 0.23651 to 0.23453, saving model to Alexnet_RMSprop.hdf5
Epoch 27/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.2466 - acc: 0.9290 - val_loss: 0.2364 - val_acc: 0.9431

Epoch 00027: val_loss did not improve from 0.23453
Epoch 28/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.2477 - acc: 0.9294 - val_loss: 0.2349 - val_acc: 0.9436

Epoch 00028: val_loss did not improve from 0.23453
Epoch 29/30
43444/43444 [==============================] - 100s 2ms/step - loss: 0.2397 - acc: 0.9304 - val_loss: 0.2362 - val_acc: 0.9433

Epoch 00029: val_loss did not improve from 0.23453
Epoch 30/30
43444/43444 [==============================] - 99s 2ms/step - loss: 0.2497 - acc: 0.9269 - val_loss: 0.2427 - val_acc: 0.9425
