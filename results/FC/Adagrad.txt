_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     
_________________________________________________________________
activation_1 (Activation)    (None, 55, 55, 96)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 27, 27, 96)        384       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   
_________________________________________________________________
activation_2 (Activation)    (None, 17, 17, 256)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    
_________________________________________________________________
activation_3 (Activation)    (None, 6, 6, 384)         0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 6, 6, 384)         1536      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   
_________________________________________________________________
activation_4 (Activation)    (None, 4, 4, 384)         0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 4, 4, 384)         1536      
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    
_________________________________________________________________
activation_5 (Activation)    (None, 2, 2, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              1052672   
_________________________________________________________________
activation_6 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
activation_7 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
_________________________________________________________________
activation_8 (Activation)    (None, 1000)              0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 1000)              0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 1000)              4000      
_________________________________________________________________
dense_4 (Dense)              (None, 38)                38038     
_________________________________________________________________
activation_9 (Activation)    (None, 38)                0         
=================================================================
Total params: 28,117,790
Trainable params: 28,096,654
Non-trainable params: 21,136
_________________________________________________________________

lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)
csv_logger = CSVLogger('Alexnet_Adagrad.csv')
early_stopper = EarlyStopping(min_delta=0.001,patience=30)
model_checkpoint = ModelCheckpoint('Alexnet_Adagrad.hdf5',monitor = 'val_loss', verbose = 1,save_best_only=True)


model.compile(loss='categorical_crossentropy',
        optimizer="Adagrad",
        metrics=['accuracy'])

model.fit(alexnet_train_data, alexnet_train_label,
              batch_size=12,
              epochs=30,
              validation_data=(alexnet_test_data,alexnet_test_label),
              shuffle=True,callbacks=[lr_reducer,csv_logger,early_stopper,model_checkpoint])
Train on 43444 samples, validate on 10861 samples
Epoch 1/30
43444/43444 [==============================] - 95s 2ms/step - loss: 2.7826 - acc: 0.2728 - val_loss: 3.0727 - val_acc: 0.2279

Epoch 00001: val_loss improved from inf to 3.07270, saving model to Alexnet_Adagrad.hdf5
Epoch 2/30
43444/43444 [==============================] - 89s 2ms/step - loss: 1.7629 - acc: 0.4961 - val_loss: 1.8858 - val_acc: 0.5008

Epoch 00002: val_loss improved from 3.07270 to 1.88584, saving model to Alexnet_Adagrad.hdf5
Epoch 3/30
43444/43444 [==============================] - 89s 2ms/step - loss: 1.2942 - acc: 0.6176 - val_loss: 1.0474 - val_acc: 0.6902

Epoch 00003: val_loss improved from 1.88584 to 1.04740, saving model to Alexnet_Adagrad.hdf5
Epoch 4/30
43444/43444 [==============================] - 89s 2ms/step - loss: 1.0180 - acc: 0.6945 - val_loss: 1.2902 - val_acc: 0.6138

Epoch 00004: val_loss did not improve from 1.04740
Epoch 5/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.8175 - acc: 0.7503 - val_loss: 0.7350 - val_acc: 0.7799

Epoch 00005: val_loss improved from 1.04740 to 0.73499, saving model to Alexnet_Adagrad.hdf5
Epoch 6/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.6824 - acc: 0.7907 - val_loss: 0.5032 - val_acc: 0.8543

Epoch 00006: val_loss improved from 0.73499 to 0.50324, saving model to Alexnet_Adagrad.hdf5
Epoch 7/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.5884 - acc: 0.8206 - val_loss: 0.5566 - val_acc: 0.8333

Epoch 00007: val_loss did not improve from 0.50324
Epoch 8/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.4983 - acc: 0.8456 - val_loss: 0.4178 - val_acc: 0.8714

Epoch 00008: val_loss improved from 0.50324 to 0.41780, saving model to Alexnet_Adagrad.hdf5
Epoch 9/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.4429 - acc: 0.8627 - val_loss: 0.4037 - val_acc: 0.8843

Epoch 00009: val_loss improved from 0.41780 to 0.40373, saving model to Alexnet_Adagrad.hdf5
Epoch 10/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.3833 - acc: 0.8809 - val_loss: 0.4146 - val_acc: 0.8734

Epoch 00010: val_loss did not improve from 0.40373
Epoch 11/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.3357 - acc: 0.8949 - val_loss: 0.2496 - val_acc: 0.9311

Epoch 00011: val_loss improved from 0.40373 to 0.24961, saving model to Alexnet_Adagrad.hdf5
Epoch 12/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.2867 - acc: 0.9102 - val_loss: 0.3173 - val_acc: 0.9081

Epoch 00012: val_loss did not improve from 0.24961
Epoch 13/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.2727 - acc: 0.9152 - val_loss: 0.3054 - val_acc: 0.9140

Epoch 00013: val_loss did not improve from 0.24961
Epoch 14/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1940 - acc: 0.9392 - val_loss: 0.2047 - val_acc: 0.9431

Epoch 00014: val_loss improved from 0.24961 to 0.20467, saving model to Alexnet_Adagrad.hdf5
Epoch 15/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.1730 - acc: 0.9455 - val_loss: 0.1915 - val_acc: 0.9457

Epoch 00015: val_loss improved from 0.20467 to 0.19146, saving model to Alexnet_Adagrad.hdf5
Epoch 16/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.1599 - acc: 0.9506 - val_loss: 1.0450 - val_acc: 0.7022

Epoch 00016: val_loss did not improve from 0.19146
Epoch 17/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.1533 - acc: 0.9535 - val_loss: 0.1573 - val_acc: 0.9560

Epoch 00017: val_loss improved from 0.19146 to 0.15732, saving model to Alexnet_Adagrad.hdf5
Epoch 18/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.1420 - acc: 0.9565 - val_loss: 0.2386 - val_acc: 0.9296

Epoch 00018: val_loss did not improve from 0.15732
Epoch 19/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.1310 - acc: 0.9597 - val_loss: 0.2921 - val_acc: 0.9168

Epoch 00019: val_loss did not improve from 0.15732
Epoch 20/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.1236 - acc: 0.9621 - val_loss: 0.1495 - val_acc: 0.9591

Epoch 00020: val_loss improved from 0.15732 to 0.14950, saving model to Alexnet_Adagrad.hdf5
Epoch 21/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1158 - acc: 0.9650 - val_loss: 0.1480 - val_acc: 0.9590

Epoch 00021: val_loss improved from 0.14950 to 0.14797, saving model to Alexnet_Adagrad.hdf5
Epoch 22/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1094 - acc: 0.9667 - val_loss: 0.1433 - val_acc: 0.9602

Epoch 00022: val_loss improved from 0.14797 to 0.14334, saving model to Alexnet_Adagrad.hdf5
Epoch 23/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1082 - acc: 0.9663 - val_loss: 0.1468 - val_acc: 0.9591

Epoch 00023: val_loss did not improve from 0.14334
Epoch 24/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1035 - acc: 0.9694 - val_loss: 0.1439 - val_acc: 0.9598

Epoch 00024: val_loss did not improve from 0.14334
Epoch 25/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.0996 - acc: 0.9697 - val_loss: 0.1397 - val_acc: 0.9611

Epoch 00025: val_loss improved from 0.14334 to 0.13965, saving model to Alexnet_Adagrad.hdf5
Epoch 26/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.0994 - acc: 0.9700 - val_loss: 0.1419 - val_acc: 0.9603

Epoch 00026: val_loss did not improve from 0.13965
Epoch 27/30
43444/43444 [==============================] - 91s 2ms/step - loss: 0.0978 - acc: 0.9704 - val_loss: 0.1427 - val_acc: 0.9599

Epoch 00027: val_loss did not improve from 0.13965
Epoch 28/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1017 - acc: 0.9696 - val_loss: 0.1410 - val_acc: 0.9613

Epoch 00028: val_loss did not improve from 0.13965
Epoch 29/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.1021 - acc: 0.9686 - val_loss: 0.1411 - val_acc: 0.9608

Epoch 00029: val_loss did not improve from 0.13965
Epoch 30/30
43444/43444 [==============================] - 90s 2ms/step - loss: 0.0998 - acc: 0.9704 - val_loss: 0.1389 - val_acc: 0.9616
