_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     
_________________________________________________________________
activation_1 (Activation)    (None, 55, 55, 96)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 27, 27, 96)        384       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   
_________________________________________________________________
activation_2 (Activation)    (None, 17, 17, 256)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    
_________________________________________________________________
activation_3 (Activation)    (None, 6, 6, 384)         0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 6, 6, 384)         1536      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   
_________________________________________________________________
activation_4 (Activation)    (None, 4, 4, 384)         0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 4, 4, 384)         1536      
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    
_________________________________________________________________
activation_5 (Activation)    (None, 2, 2, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              1052672   
_________________________________________________________________
activation_6 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
activation_7 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
_________________________________________________________________
activation_8 (Activation)    (None, 1000)              0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 1000)              0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 1000)              4000      
_________________________________________________________________
dense_4 (Dense)              (None, 38)                38038     
_________________________________________________________________
activation_9 (Activation)    (None, 38)                0         
=================================================================
Total params: 28,117,790
Trainable params: 28,096,654
Non-trainable params: 21,136
_________________________________________________________________

lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)
csv_logger = CSVLogger('Alexnet_Adamax.csv')
early_stopper = EarlyStopping(min_delta=0.001,patience=30)
model_checkpoint = ModelCheckpoint('Alexnet_Adamax.hdf5',monitor = 'val_loss', verbose = 1,save_best_only=True)


model.compile(loss='categorical_crossentropy',
        optimizer="Adamax",
        metrics=['accuracy'])

model.fit(alexnet_train_data, alexnet_train_label,
              batch_size=12,
              epochs=30,
              validation_data=(alexnet_test_data,alexnet_test_label),
              shuffle=True,callbacks=[lr_reducer,csv_logger,early_stopper,model_checkpoint])
Train on 43444 samples, validate on 10861 samples
Epoch 1/30
43444/43444 [==============================] - 97s 2ms/step - loss: 2.3913 - acc: 0.3691 - val_loss: 3.4033 - val_acc: 0.2985

Epoch 00001: val_loss improved from inf to 3.40332, saving model to Alexnet_Adamax.hdf5
Epoch 2/30
43444/43444 [==============================] - 95s 2ms/step - loss: 1.3285 - acc: 0.6067 - val_loss: 1.7756 - val_acc: 0.5021

Epoch 00002: val_loss improved from 3.40332 to 1.77562, saving model to Alexnet_Adamax.hdf5
Epoch 3/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.9543 - acc: 0.7101 - val_loss: 1.3570 - val_acc: 0.6008

Epoch 00003: val_loss improved from 1.77562 to 1.35703, saving model to Alexnet_Adamax.hdf5
Epoch 4/30
43444/43444 [==============================] - 95s 2ms/step - loss: 0.7577 - acc: 0.7664 - val_loss: 0.7533 - val_acc: 0.7661

Epoch 00004: val_loss improved from 1.35703 to 0.75333, saving model to Alexnet_Adamax.hdf5
Epoch 5/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.6314 - acc: 0.8031 - val_loss: 0.6629 - val_acc: 0.7942

Epoch 00005: val_loss improved from 0.75333 to 0.66289, saving model to Alexnet_Adamax.hdf5
Epoch 6/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.5216 - acc: 0.8391 - val_loss: 0.6890 - val_acc: 0.7918

Epoch 00006: val_loss did not improve from 0.66289
Epoch 7/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.4509 - acc: 0.8593 - val_loss: 0.5487 - val_acc: 0.8298

Epoch 00007: val_loss improved from 0.66289 to 0.54868, saving model to Alexnet_Adamax.hdf5
Epoch 8/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.3804 - acc: 0.8805 - val_loss: 0.4003 - val_acc: 0.8801

Epoch 00008: val_loss improved from 0.54868 to 0.40026, saving model to Alexnet_Adamax.hdf5
Epoch 9/30
43444/43444 [==============================] - 96s 2ms/step - loss: 0.3265 - acc: 0.8975 - val_loss: 0.8329 - val_acc: 0.7619

Epoch 00009: val_loss did not improve from 0.40026
Epoch 10/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.2909 - acc: 0.9066 - val_loss: 0.3925 - val_acc: 0.8880

Epoch 00010: val_loss improved from 0.40026 to 0.39245, saving model to Alexnet_Adamax.hdf5
Epoch 11/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.2752 - acc: 0.9113 - val_loss: 0.5018 - val_acc: 0.8515

Epoch 00011: val_loss did not improve from 0.39245
Epoch 12/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.2454 - acc: 0.9219 - val_loss: 0.5069 - val_acc: 0.8592

Epoch 00012: val_loss did not improve from 0.39245
Epoch 13/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.1553 - acc: 0.9502 - val_loss: 0.2573 - val_acc: 0.9298

Epoch 00013: val_loss improved from 0.39245 to 0.25728, saving model to Alexnet_Adamax.hdf5
Epoch 14/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.1281 - acc: 0.9591 - val_loss: 0.2071 - val_acc: 0.9478

Epoch 00014: val_loss improved from 0.25728 to 0.20711, saving model to Alexnet_Adamax.hdf5
Epoch 15/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.1111 - acc: 0.9639 - val_loss: 0.1887 - val_acc: 0.9515

Epoch 00015: val_loss improved from 0.20711 to 0.18871, saving model to Alexnet_Adamax.hdf5
Epoch 16/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.0960 - acc: 0.9685 - val_loss: 0.1990 - val_acc: 0.9477

Epoch 00016: val_loss did not improve from 0.18871
Epoch 17/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0907 - acc: 0.9701 - val_loss: 0.1801 - val_acc: 0.9527

Epoch 00017: val_loss improved from 0.18871 to 0.18012, saving model to Alexnet_Adamax.hdf5
Epoch 18/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0824 - acc: 0.9728 - val_loss: 0.1756 - val_acc: 0.9548

Epoch 00018: val_loss improved from 0.18012 to 0.17555, saving model to Alexnet_Adamax.hdf5
Epoch 19/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0761 - acc: 0.9753 - val_loss: 0.1869 - val_acc: 0.9507

Epoch 00019: val_loss did not improve from 0.17555
Epoch 20/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0690 - acc: 0.9779 - val_loss: 0.1854 - val_acc: 0.9512

Epoch 00020: val_loss did not improve from 0.17555
Epoch 21/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.0546 - acc: 0.9824 - val_loss: 0.1661 - val_acc: 0.9600

Epoch 00021: val_loss improved from 0.17555 to 0.16609, saving model to Alexnet_Adamax.hdf5
Epoch 22/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.0493 - acc: 0.9843 - val_loss: 0.1458 - val_acc: 0.9639

Epoch 00022: val_loss improved from 0.16609 to 0.14577, saving model to Alexnet_Adamax.hdf5
Epoch 23/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0460 - acc: 0.9851 - val_loss: 0.1700 - val_acc: 0.9603

Epoch 00023: val_loss did not improve from 0.14577
Epoch 24/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0432 - acc: 0.9864 - val_loss: 0.1561 - val_acc: 0.9623

Epoch 00024: val_loss did not improve from 0.14577
Epoch 25/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0413 - acc: 0.9872 - val_loss: 0.1566 - val_acc: 0.9632

Epoch 00025: val_loss did not improve from 0.14577
Epoch 26/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0381 - acc: 0.9883 - val_loss: 0.1574 - val_acc: 0.9646

Epoch 00026: val_loss did not improve from 0.14577
Epoch 27/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0389 - acc: 0.9876 - val_loss: 0.1535 - val_acc: 0.9645

Epoch 00027: val_loss did not improve from 0.14577
Epoch 28/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0381 - acc: 0.9874 - val_loss: 0.1585 - val_acc: 0.9632

Epoch 00028: val_loss did not improve from 0.14577
Epoch 29/30
43444/43444 [==============================] - 97s 2ms/step - loss: 0.0374 - acc: 0.9887 - val_loss: 0.1554 - val_acc: 0.9645

Epoch 00029: val_loss did not improve from 0.14577
Epoch 30/30
43444/43444 [==============================] - 98s 2ms/step - loss: 0.0374 - acc: 0.9885 - val_loss: 0.1511 - val_acc: 0.9634

Epoch 00030: val_loss did not improve from 0.14577
