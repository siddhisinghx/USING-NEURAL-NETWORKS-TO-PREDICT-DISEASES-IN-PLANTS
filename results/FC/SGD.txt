_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     
_________________________________________________________________
activation_1 (Activation)    (None, 55, 55, 96)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 27, 27, 96)        384       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   
_________________________________________________________________
activation_2 (Activation)    (None, 17, 17, 256)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    
_________________________________________________________________
activation_3 (Activation)    (None, 6, 6, 384)         0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 6, 6, 384)         1536      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   
_________________________________________________________________
activation_4 (Activation)    (None, 4, 4, 384)         0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 4, 4, 384)         1536      
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    
_________________________________________________________________
activation_5 (Activation)    (None, 2, 2, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              1052672   
_________________________________________________________________
activation_6 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
activation_7 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
_________________________________________________________________
activation_8 (Activation)    (None, 1000)              0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 1000)              0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 1000)              4000      
_________________________________________________________________
dense_4 (Dense)              (None, 38)                38038     
_________________________________________________________________
activation_9 (Activation)    (None, 38)                0         
=================================================================
Total params: 28,117,790
Trainable params: 28,096,654
Non-trainable params: 21,136
_________________________________________________________________

model.fit(alexnet_train_data, alexnet_train_label,
              batch_size=12,
              epochs=30,
              validation_data=(alexnet_test_data,alexnet_test_label),
              shuffle=True,callbacks=[lr_reducer,csv_logger,early_stopper,model_checkpoint])
Train on 43444 samples, validate on 10861 samples
Epoch 1/30
43444/43444 [==============================] - 80s 2ms/step - loss: 2.3410 - acc: 0.3840 - val_loss: 4.0827 - val_acc: 0.1752

Epoch 00001: val_loss improved from inf to 4.08274, saving model to Alexnet_SGD.hdf5
Epoch 2/30
43444/43444 [==============================] - 77s 2ms/step - loss: 1.3265 - acc: 0.6109 - val_loss: 1.0216 - val_acc: 0.6884

Epoch 00002: val_loss improved from 4.08274 to 1.02155, saving model to Alexnet_SGD.hdf5
Epoch 3/30
43444/43444 [==============================] - 78s 2ms/step - loss: 0.9646 - acc: 0.7068 - val_loss: 4.9796 - val_acc: 0.2878

Epoch 00003: val_loss did not improve from 1.02155
Epoch 4/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.7733 - acc: 0.7617 - val_loss: 0.8155 - val_acc: 0.7440

Epoch 00004: val_loss improved from 1.02155 to 0.81549, saving model to Alexnet_SGD.hdf5
Epoch 5/30
43444/43444 [==============================] - 78s 2ms/step - loss: 0.6298 - acc: 0.8017 - val_loss: 0.7901 - val_acc: 0.7666

Epoch 00005: val_loss improved from 0.81549 to 0.79008, saving model to Alexnet_SGD.hdf5
Epoch 6/30
43444/43444 [==============================] - 78s 2ms/step - loss: 0.5264 - acc: 0.8349 - val_loss: 2.3064 - val_acc: 0.4986

Epoch 00006: val_loss did not improve from 0.79008
Epoch 7/30
43444/43444 [==============================] - 78s 2ms/step - loss: 0.4574 - acc: 0.8542 - val_loss: 2.5995 - val_acc: 0.5212

Epoch 00007: val_loss did not improve from 0.79008
Epoch 8/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.3098 - acc: 0.9004 - val_loss: 0.2636 - val_acc: 0.9273

Epoch 00008: val_loss improved from 0.79008 to 0.26362, saving model to Alexnet_SGD.hdf5
Epoch 9/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.2618 - acc: 0.9146 - val_loss: 0.3106 - val_acc: 0.9046

Epoch 00009: val_loss did not improve from 0.26362
Epoch 10/30
43444/43444 [==============================] - 76s 2ms/step - loss: 0.2397 - acc: 0.9231 - val_loss: 0.2159 - val_acc: 0.9390

Epoch 00010: val_loss improved from 0.26362 to 0.21585, saving model to Alexnet_SGD.hdf5
Epoch 11/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.2142 - acc: 0.9298 - val_loss: 0.2219 - val_acc: 0.9395

Epoch 00011: val_loss did not improve from 0.21585
Epoch 12/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1945 - acc: 0.9362 - val_loss: 0.2831 - val_acc: 0.9193

Epoch 00012: val_loss did not improve from 0.21585
Epoch 13/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1597 - acc: 0.9478 - val_loss: 0.2360 - val_acc: 0.9324

Epoch 00013: val_loss did not improve from 0.21585
Epoch 14/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1494 - acc: 0.9509 - val_loss: 0.1760 - val_acc: 0.9515

Epoch 00014: val_loss improved from 0.21585 to 0.17605, saving model to Alexnet_SGD.hdf5
Epoch 15/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1385 - acc: 0.9552 - val_loss: 0.1937 - val_acc: 0.9484

Epoch 00015: val_loss did not improve from 0.17605
Epoch 16/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1341 - acc: 0.9568 - val_loss: 0.1802 - val_acc: 0.9502

Epoch 00016: val_loss did not improve from 0.17605
Epoch 17/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1227 - acc: 0.9607 - val_loss: 0.1725 - val_acc: 0.9568

Epoch 00017: val_loss improved from 0.17605 to 0.17249, saving model to Alexnet_SGD.hdf5
Epoch 18/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1178 - acc: 0.9629 - val_loss: 0.1599 - val_acc: 0.9568

Epoch 00018: val_loss improved from 0.17249 to 0.15993, saving model to Alexnet_SGD.hdf5
Epoch 19/30
43444/43444 [==============================] - 78s 2ms/step - loss: 0.1143 - acc: 0.9628 - val_loss: 0.1661 - val_acc: 0.9552

Epoch 00019: val_loss did not improve from 0.15993
Epoch 20/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1150 - acc: 0.9643 - val_loss: 0.1842 - val_acc: 0.9514

Epoch 00020: val_loss did not improve from 0.15993
Epoch 21/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1135 - acc: 0.9638 - val_loss: 0.1544 - val_acc: 0.9588

Epoch 00021: val_loss improved from 0.15993 to 0.15438, saving model to Alexnet_SGD.hdf5
Epoch 22/30
43444/43444 [==============================] - 78s 2ms/step - loss: 0.1115 - acc: 0.9648 - val_loss: 0.1602 - val_acc: 0.9585

Epoch 00022: val_loss did not improve from 0.15438
Epoch 23/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1098 - acc: 0.9657 - val_loss: 0.1677 - val_acc: 0.9582

Epoch 00023: val_loss did not improve from 0.15438
Epoch 24/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1040 - acc: 0.9666 - val_loss: 0.1568 - val_acc: 0.9599

Epoch 00024: val_loss did not improve from 0.15438
Epoch 25/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1068 - acc: 0.9667 - val_loss: 0.1666 - val_acc: 0.9586

Epoch 00025: val_loss did not improve from 0.15438
Epoch 26/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1110 - acc: 0.9637 - val_loss: 0.1647 - val_acc: 0.9597

Epoch 00026: val_loss did not improve from 0.15438
Epoch 27/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1069 - acc: 0.9663 - val_loss: 0.1623 - val_acc: 0.9581

Epoch 00027: val_loss did not improve from 0.15438
Epoch 28/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1071 - acc: 0.9659 - val_loss: 0.1593 - val_acc: 0.9590

Epoch 00028: val_loss did not improve from 0.15438
Epoch 29/30
43444/43444 [==============================] - 77s 2ms/step - loss: 0.1072 - acc: 0.9657 - val_loss: 0.1685 - val_acc: 0.9585

Epoch 00029: val_loss did not improve from 0.15438
Epoch 30/30
43444/43444 [==============================] - 80s 2ms/step - loss: 0.1038 - acc: 0.9672 - val_loss: 0.1565 - val_acc: 0.9598

Epoch 00030: val_loss did not improve from 0.15438
Out[5]: <keras.callbacks.History at 0x7f415a6f5a20>

