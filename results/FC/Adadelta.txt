_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     
_________________________________________________________________
activation_1 (Activation)    (None, 55, 55, 96)        0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 27, 27, 96)        384       
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   
_________________________________________________________________
activation_2 (Activation)    (None, 17, 17, 256)       0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 8, 8, 256)         1024      
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    
_________________________________________________________________
activation_3 (Activation)    (None, 6, 6, 384)         0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 6, 6, 384)         1536      
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   
_________________________________________________________________
activation_4 (Activation)    (None, 4, 4, 384)         0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 4, 4, 384)         1536      
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    
_________________________________________________________________
activation_5 (Activation)    (None, 2, 2, 256)         0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         
_________________________________________________________________
batch_normalization_5 (Batch (None, 1, 1, 256)         1024      
_________________________________________________________________
flatten_1 (Flatten)          (None, 256)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 4096)              1052672   
_________________________________________________________________
activation_6 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_6 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_2 (Dense)              (None, 4096)              16781312  
_________________________________________________________________
activation_7 (Activation)    (None, 4096)              0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 4096)              0         
_________________________________________________________________
batch_normalization_7 (Batch (None, 4096)              16384     
_________________________________________________________________
dense_3 (Dense)              (None, 1000)              4097000   
_________________________________________________________________
activation_8 (Activation)    (None, 1000)              0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 1000)              0         
_________________________________________________________________
batch_normalization_8 (Batch (None, 1000)              4000      
_________________________________________________________________
dense_4 (Dense)              (None, 38)                38038     
_________________________________________________________________
activation_9 (Activation)    (None, 38)                0         
=================================================================
Total params: 28,117,790
Trainable params: 28,096,654
Non-trainable params: 21,136
_________________________________________________________________

lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)
csv_logger = CSVLogger('Alexnet_Adadelta.csv')
early_stopper = EarlyStopping(min_delta=0.001,patience=30)
model_checkpoint = ModelCheckpoint('Alexnet_Adadelta.hdf5',monitor = 'val_loss', verbose = 1,save_best_only=True)


model.compile(loss='categorical_crossentropy',
        optimizer="Adadelta",
        metrics=['accuracy'])

model.fit(alexnet_train_data, alexnet_train_label,
              batch_size=12,
              epochs=30,
              validation_data=(alexnet_test_data,alexnet_test_label),
              shuffle=True,callbacks=[lr_reducer,csv_logger,early_stopper,model_checkpoint])
Train on 43444 samples, validate on 10861 samples
Epoch 1/30
43444/43444 [==============================] - 116s 3ms/step - loss: 2.5788 - acc: 0.3214 - val_loss: 2.0440 - val_acc: 0.4547

Epoch 00001: val_loss improved from inf to 2.04403, saving model to Alexnet_Adadelta.hdf5
Epoch 2/30
43444/43444 [==============================] - 119s 3ms/step - loss: 1.6356 - acc: 0.5303 - val_loss: 2.2583 - val_acc: 0.4929

Epoch 00002: val_loss did not improve from 2.04403
Epoch 3/30
43444/43444 [==============================] - 120s 3ms/step - loss: 1.2628 - acc: 0.6337 - val_loss: 2.0710 - val_acc: 0.4962

Epoch 00003: val_loss did not improve from 2.04403
Epoch 4/30
43444/43444 [==============================] - 120s 3ms/step - loss: 0.9017 - acc: 0.7324 - val_loss: 1.2073 - val_acc: 0.6592

Epoch 00004: val_loss improved from 2.04403 to 1.20729, saving model to Alexnet_Adadelta.hdf5
Epoch 5/30
43444/43444 [==============================] - 121s 3ms/step - loss: 0.8005 - acc: 0.7614 - val_loss: 0.8630 - val_acc: 0.7640

Epoch 00005: val_loss improved from 1.20729 to 0.86305, saving model to Alexnet_Adadelta.hdf5
Epoch 6/30
43444/43444 [==============================] - 123s 3ms/step - loss: 0.7118 - acc: 0.7893 - val_loss: 0.6412 - val_acc: 0.8273

Epoch 00006: val_loss improved from 0.86305 to 0.64122, saving model to Alexnet_Adadelta.hdf5
Epoch 7/30
43444/43444 [==============================] - 123s 3ms/step - loss: 0.6338 - acc: 0.8104 - val_loss: 0.9945 - val_acc: 0.7442

Epoch 00007: val_loss did not improve from 0.64122
Epoch 8/30
43444/43444 [==============================] - 124s 3ms/step - loss: 0.5776 - acc: 0.8290 - val_loss: 0.6640 - val_acc: 0.8255

Epoch 00008: val_loss did not improve from 0.64122
Epoch 9/30
43444/43444 [==============================] - 128s 3ms/step - loss: 0.4592 - acc: 0.8636 - val_loss: 0.5132 - val_acc: 0.8698

Epoch 00009: val_loss improved from 0.64122 to 0.51318, saving model to Alexnet_Adadelta.hdf5
Epoch 10/30
43444/43444 [==============================] - 130s 3ms/step - loss: 0.4174 - acc: 0.8746 - val_loss: 0.4648 - val_acc: 0.8835

Epoch 00010: val_loss improved from 0.51318 to 0.46481, saving model to Alexnet_Adadelta.hdf5
Epoch 11/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.4015 - acc: 0.8798 - val_loss: 0.3770 - val_acc: 0.9111

Epoch 00011: val_loss improved from 0.46481 to 0.37702, saving model to Alexnet_Adadelta.hdf5
Epoch 12/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.3754 - acc: 0.8879 - val_loss: 0.3943 - val_acc: 0.9009

Epoch 00012: val_loss did not improve from 0.37702
Epoch 13/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.3561 - acc: 0.8933 - val_loss: 0.3601 - val_acc: 0.9119

Epoch 00013: val_loss improved from 0.37702 to 0.36010, saving model to Alexnet_Adadelta.hdf5
Epoch 14/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.3450 - acc: 0.8960 - val_loss: 0.3286 - val_acc: 0.9166

Epoch 00014: val_loss improved from 0.36010 to 0.32860, saving model to Alexnet_Adadelta.hdf5
Epoch 15/30
43444/43444 [==============================] - 127s 3ms/step - loss: 0.3248 - acc: 0.9027 - val_loss: 0.3095 - val_acc: 0.9236

Epoch 00015: val_loss improved from 0.32860 to 0.30946, saving model to Alexnet_Adadelta.hdf5
Epoch 16/30
43444/43444 [==============================] - 128s 3ms/step - loss: 0.3100 - acc: 0.9066 - val_loss: 0.4098 - val_acc: 0.8938

Epoch 00016: val_loss did not improve from 0.30946
Epoch 17/30
43444/43444 [==============================] - 130s 3ms/step - loss: 0.2906 - acc: 0.9121 - val_loss: 0.3467 - val_acc: 0.9118

Epoch 00017: val_loss did not improve from 0.30946
Epoch 18/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.2622 - acc: 0.9213 - val_loss: 0.3153 - val_acc: 0.9231

Epoch 00018: val_loss did not improve from 0.30946
Epoch 19/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.2465 - acc: 0.9257 - val_loss: 0.2808 - val_acc: 0.9286

Epoch 00019: val_loss improved from 0.30946 to 0.28082, saving model to Alexnet_Adadelta.hdf5
Epoch 20/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.2381 - acc: 0.9290 - val_loss: 0.2928 - val_acc: 0.9303

Epoch 00020: val_loss did not improve from 0.28082
Epoch 21/30
43444/43444 [==============================] - 127s 3ms/step - loss: 0.2294 - acc: 0.9323 - val_loss: 0.2858 - val_acc: 0.9280

Epoch 00021: val_loss did not improve from 0.28082
Epoch 22/30
43444/43444 [==============================] - 127s 3ms/step - loss: 0.2155 - acc: 0.9342 - val_loss: 0.2879 - val_acc: 0.9316

Epoch 00022: val_loss did not improve from 0.28082
Epoch 23/30
43444/43444 [==============================] - 128s 3ms/step - loss: 0.2197 - acc: 0.9336 - val_loss: 0.2968 - val_acc: 0.9286

Epoch 00023: val_loss did not improve from 0.28082
Epoch 24/30
43444/43444 [==============================] - 129s 3ms/step - loss: 0.2176 - acc: 0.9339 - val_loss: 0.2819 - val_acc: 0.9309

Epoch 00024: val_loss did not improve from 0.28082
Epoch 25/30
43444/43444 [==============================] - 128s 3ms/step - loss: 0.2171 - acc: 0.9329 - val_loss: 0.2891 - val_acc: 0.9304

Epoch 00025: val_loss did not improve from 0.28082
Epoch 26/30
43444/43444 [==============================] - 124s 3ms/step - loss: 0.2167 - acc: 0.9338 - val_loss: 0.2772 - val_acc: 0.9309

Epoch 00026: val_loss improved from 0.28082 to 0.27717, saving model to Alexnet_Adadelta.hdf5
Epoch 27/30
43444/43444 [==============================] - 125s 3ms/step - loss: 0.2179 - acc: 0.9342 - val_loss: 0.2757 - val_acc: 0.9325

Epoch 00027: val_loss improved from 0.27717 to 0.27573, saving model to Alexnet_Adadelta.hdf5
Epoch 28/30
43444/43444 [==============================] - 124s 3ms/step - loss: 0.2152 - acc: 0.9343 - val_loss: 0.2728 - val_acc: 0.9323

Epoch 00028: val_loss improved from 0.27573 to 0.27280, saving model to Alexnet_Adadelta.hdf5
Epoch 29/30
43444/43444 [==============================] - 123s 3ms/step - loss: 0.2085 - acc: 0.9355 - val_loss: 0.2810 - val_acc: 0.9300

Epoch 00029: val_loss did not improve from 0.27280
Epoch 30/30
43444/43444 [==============================] - 122s 3ms/step - loss: 0.2079 - acc: 0.9369 - val_loss: 0.2874 - val_acc: 0.9317

Epoch 00030: val_loss did not improve from 0.27280
